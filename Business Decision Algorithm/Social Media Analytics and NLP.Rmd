---
title: "Social Media Analytics and NLP"
author: "Bhoomika John Pedely"
date: "2023-09-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tm)
library(RTextTools)
library(wordcloud)
library(ggplot2)
text <- read.csv(url("http://data.mishra.us/files//gettysberg.csv"))
corpus <- VCorpus(VectorSource(text$text))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
# a function to clean /,@,\\,|
corpus <- tm_map(corpus, toSpace, "/|@|\\|")
corpus<- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords(kind="en"))
# Run with and without stop words to see how word cloud changes
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 2,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"),scale=c(3, 0.7))
```
```{r}
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)
corpus<- read.csv(url("http://data.mishra.us/files//twitter_corpus.csv"))
# create a corpus from character vectors from the imported .csv file
corpus <- VCorpus(VectorSource(corpus$text))
# now we are Creating a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
# remove dtm rows with no words i.e., tweets that have no words
# after preprocessing text.
dtm <- dtm[rowTotals> 0, ]
```

```{r}
set.seed(234)
lda <- LDA(dtm, k = 2, method = "Gibbs", control = NULL)
# the method used for fitting can be either "VEM" or "Gibbs"
topics <- tidy(lda, matrix = "beta") # beta is the topic-word density
top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>% # top_n picks 10 topics.
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
```
```{r}
set.seed(234)
lda <- LDA(dtm, k = 3, method = "Gibbs", control = NULL)
# the method used for fitting can be either "VEM" or "Gibbs"
topics <- tidy(lda, matrix = "beta")
# beta is the topic-word density
top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>% # top_n picks 10 topics.
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
```
```{r}
set.seed(234)
lda <- LDA(dtm, k = 4, method = "Gibbs", control = NULL)
# the method used for fitting can be either "VEM" or "Gibbs"
topics <- tidy(lda, matrix = "beta") # beta is the topic-word density
top_terms <- topics %>%
group_by(topic) %>%
  top_n(10, beta) %>% # top_n picks 10 topics.
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
```
```{r}
corpus<- read.csv(url("http://data.mishra.us/files//twitter_corpus.csv"))
# create a corpus from character vectors from the imported .csv file
corpus <- VCorpus(VectorSource(corpus$text))
# now we are Creating a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)
tidy_tweets <- tidy(dtm) # here data is in dtm format.
#We need to convert it into tidy format using broom
tweet_sentiments <- tidy_tweets %>%
  inner_join(get_sentiments("afinn"), by = c(term = "word"))
# we can use bing, afinn or nrc
tweet_sentiments
```

